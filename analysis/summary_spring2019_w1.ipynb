{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick up from what I left last quarter, some summary..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to `GTEx` dataset directly\n",
    "Result: `betanmf` (just multiplicative update while replacing 0 with small number in iteration) is \"fastest\" (not rigorous) while `ccd` has highest poisson likelihood. \n",
    "\n",
    "Todo and question:\n",
    "* run `betanmf` longer to see if it can do even better\n",
    "\n",
    "* more systematically, compute loss during computation and see convergence behavior. Start with `betanmf` and `ccd`\n",
    "\n",
    "* Read about analysis of their convergence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to GTEx-sim dataset\n",
    "I generate simulated dataset from `L,F` of `GTEx` fitted by `nnmf`. \n",
    "Result: `nnmf` beats the oracle while other methods are far from that. The champion on original dataset, ccd, doesn't do well at all!\n",
    "\n",
    "Todo and question:\n",
    "* It is very strange that `nnmf` is the only one does well. Is is just coincidence? Maybe use the fit from say `maptpx` to see. \n",
    "\n",
    "* make more realistic simulation: using negative binomial. But there are some technical issue here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of GTEx and GTEx-sim results\n",
    "The median of poisson loglikelihood are close but the mean are very different. This happens when using Frobenius norm as objective function. Taking a look at the latter tells me that the worst fit (which happens to be the biggest count) contribute a lot to the loss!\n",
    "\n",
    "Todo and question:\n",
    "* Is the poisson assumption still valid here? Do we need to remove some crazy outliers? (see plot in  https://zihao12.github.io/topics-simulation-experiments/Investigate_square_error.html)\n",
    "* If we want to keep and capture that signal, do we need to use a penalized term (may even lead to better fit by bypassing the bad local minimum? just guessing: start from a more average factor, it might be hard to become a sparse one?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How effective is \"sketching\"\n",
    "I experiment with `rsvd` which first projects high dimensional data into low dimension, apply nonegative matrix factorization in low dimensional space, then projects back. It is indeed very fast and accuracy isn't bad. It is designed for model with normal assumption. One direct way to apply to Poisson data is to use it in initialization.\n",
    "\n",
    "Todo and question:\n",
    "* Can we use the sketching trick for Poisson data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some good resources:\n",
    "https://www.cs.purdue.edu/homes/dgleich/conf/slides/mmm2012/dhillon.pdf good summary of NMF algorithms, particularly `ccd` and `gcd` (latter being \"greedy coordinate descent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
